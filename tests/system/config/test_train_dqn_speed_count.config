####################################################
################## Train parameters ################
####################################################
[train_args]
network = intersection
experiment_time = 5000
experiment_save_agent = True
experiment_save_agent_interval = 25
experiment_seed = 77
sumo_render = False
sumo_emission = False
tls_type = rl
demand_type = constant
demand_mode = step



####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
discount_factor = 0.98
action_space = 'discrete'

#################### State space ###################
features = ('speed', 'count')
# features = ('speed_score', 'count')
# features = ('delay',)
# features = ('delay', 'lag[delay]')
# features = ('waiting_time',)
# features = ('queue', 'lag[queue]')
# features = ('flow',)
# features = ('pressure',)
# features = ('average_pressure',)

category_average_pressures = [-5, -3, 0, 3, 5, 10]
category_counts = [5, 10, 15, 20, 25, 30]
category_delays = [5, 10, 25, 50, 75, 100]
category_waiting_times = [5, 10, 25, 50, 75, 100]
category_flows = [1, 3, 10, 15, 20, 25]
category_queues = [1, 3, 10, 15, 20, 25]
category_times = [5, 10, 13, 17, 19, 21]
category_pressures = [-5, -3, 0, 3, 5, 10]
category_speed_scores = [3, 5, 7, 9, 11, 13]
category_speeds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]
normalize_velocities = True
normalize_vehicles = False
discretize_state_space = False
time_period = None

###################### Reward ######################
reward = 'reward_min_speed_delta'
# reward = 'reward_max_speed_score'
# reward = 'reward_min_delay'
# reward = 'reward_max_delay_reduction'
# reward = 'reward_min_waiting_time'
# reward = 'reward_min_queue_squared'
# reward = 'reward_max_flow'
# reward = 'reward_min_pressure'
# reward = 'reward_min_average_pressure'
 

reward_rescale = 0.01
velocity_threshold = 0.1



####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN

#################### Q-learning ####################
[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 500
replay_buffer_batch_size = 64
replay_buffer_warm_up = 0

######################## DQN #######################
[dqn_args]
learning_rate = 1e-3
n_step = 5
batch_size = 128
target_update_period = 100
min_replay_size = 5000
max_replay_size = 50000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
samples_per_insert = 128.0
prefetch_size = 1

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
# (state -> torso net -> head net -> Q-values).
torso_layers = [10]
head_layers = [10]

####################### R2D2 #######################
[r2d2_args]
learning_rate = 1e-3
burn_in_length = 5
trace_length = 15
replay_period = 5
batch_size = 100
target_update_period = 100
min_replay_size = 1000
max_replay_size = 20000
importance_sampling_exponent = 0.2
priority_exponent = 0.6
max_priority_weight = 0.9
samples_per_insert = 100.0
prefetch_size = 1
store_lstm_state = True

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
rnn_hidden_size = 10
head_layers = [10]

####################### DDPG #######################
[ddpg_args]
batch_size = 128
prefetch_size = 1
target_update_period = 100
min_replay_size = 1000
max_replay_size = 30000
samples_per_insert = 128.0
n_step = 5
clipping = True

# Exploration parameters.
sigma_init = 0.4
sigma_final = 0.01
sigma_schedule_timesteps = 45000

# Neural networks parameters.
policy_layers = [10, 10]
critic_layers = [10, 10]
