####################################################
################## Train parameters ################
####################################################
[train_args]
network = grid
experiment_time = 259200
experiment_save_agent = True
experiment_save_agent_interval = 86400

#If train.config debug is True, debug parameters will be used instead.
debug = False
debug_experiment_time = 259200
debug_experiment_save_agent = True
debug_experiment_save_agent_interval = 86400


# For multiple runs, seeds are set using run.config
experiment_seed = None
sumo_render = False
sumo_emission = False
tls_type = cg
#tls_type = centralized
#tls_type = rl
demand_type = constant
demand_mode = step

####################################################
################## MDP parameters ##################
####################################################
[mdp_args]
discount_factor = 0.90
action_space = 'discrete'

#################### State space ###################
# features = ('count',)
# features = ('speed', 'count')
# features = ('speed_score', 'count')
features = ('delay',)
# features = ('delay', 'lag[delay]')
# features = ('waiting_time',)
# features = ('queue',)
# features = ('queue', 'lag[queue]')
# features = ('flow',)
# features = ('pressure',)
# features = ('average_pressure',)

category_times = [6, 10, 17, 19, 21]

normalize_velocities = True
normalize_vehicles = False
discretize_state_space = False
time_period = None
# time_period = 3600

###################### Reward ######################
# reward = 'reward_min_speed_delta'
# reward = 'reward_max_speed_score'
reward = 'reward_min_delay'
# reward = 'reward_max_delay_reduction'
# reward = 'reward_min_waiting_time'
# reward = 'reward_min_queue'
# reward = 'reward_min_queue_squared'
# reward = 'reward_max_flow'
# reward = 'reward_min_pressure'
# reward = 'reward_min_average_pressure'
 
reward_rescale = 0.01
velocity_threshold = 0.1

####################################################
################ Agent's parameters ################
####################################################
[agent_type]
agent_type = DQN

#################### Q-learning ####################
[ql_args]
lr_decay_power_coef = 0.66
eps_decay_power_coef = 1
choice_type = eps-greedy
replay_buffer = True
replay_buffer_size = 20000
replay_buffer_batch_size = 128
replay_buffer_warm_up = 2000

######################## DQN #######################
[dqn_args]
learning_rate = 1e-3
n_step = 5
batch_size = 128
target_update_period = 100
min_replay_size = 17280
max_replay_size = 86400
importance_sampling_exponent = 0.9
priority_exponent = 0.6
samples_per_insert = 128.0
prefetch_size = 1
max_gradient_norm = None

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 129000

# Neural network parameters.
# (state -> torso net -> head net -> Q-values).
torso_layers = [8, 16]
head_layers = [8]


####################### R2D2 #######################
[r2d2_args]
learning_rate = 1e-3
burn_in_length = 5
trace_length = 15
replay_period = 5
batch_size = 128
target_update_period = 100
min_replay_size = 1000
max_replay_size = 20000
importance_sampling_exponent = 0.9
priority_exponent = 0.6
max_priority_weight = 0.9
samples_per_insert = 128.0
prefetch_size = 1
store_lstm_state = True

# Epsilon-greedy policy parameters.
epsilon_init = 1.0
epsilon_final = 0.01
epsilon_schedule_timesteps = 45000

# Neural network parameters.
rnn_hidden_size = 10
head_layers = [10]

####################### DDPG #######################
[ddpg_args]
batch_size = 128
prefetch_size = 1
target_update_period = 100
min_replay_size = 5000
max_replay_size = 50000
samples_per_insert = 128.0
n_step = 5
clipping = True

# Exploration parameters.
sigma_init = 0.2
sigma_final = 0.01
sigma_schedule_timesteps = 45000

# Neural networks parameters.
policy_layers = [16, 32, 16]
critic_layers = [16, 32, 16]
